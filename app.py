import streamlit as st
import pandas as pd
import json
import io
import base64
import os
import re
import datetime
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import plotly.express as px

# --- 1. CONFIGURATION & BRANDING ---
COMPANY_NAME = "Orient Scale Solutions"
WEBSITE_URL = "https://www.orientscale.ma"
PHONE_NUMBER = "0661572700"
LOGO_PATH = "logo.png"

# --- ğŸ› ï¸ JSON KEY MAPPING ğŸ› ï¸ ---
JSON_KEY_PLATE = "plate"
JSON_KEY_PRODUCT = "product"
JSON_KEY_CLIENT = "client"
JSON_KEY_DRIVER = "driver"
JSON_KEY_WEIGHT = "net"
JSON_KEY_PRICE = "price"
JSON_KEY_DATE = "date_out"

# --- ğŸ› ï¸ CUSTOM FIELDS CONFIGURATION ğŸ› ï¸ ---
UI_NAME_1 = "Destination"
JSON_KEY_1 = "ex1"

UI_NAME_2 = "Source"
JSON_KEY_2 = "ex2"

UI_NAME_3 = "Remorque"
JSON_KEY_3 = "ex3"

# Drive Config
FOLDER_ID = "115OinwLcQMYZ2l50qMEACnCt-9pXiy1o"
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']

# --- PAGE CONFIG ---
st.set_page_config(page_title=COMPANY_NAME, layout="wide", page_icon="âš–ï¸")

# --- 2. TRANSLATION DICTIONARY ---
TRANSLATIONS = {
Â  Â  "FR": {
Â  Â  Â  Â  "title": "Tableau de Bord des OpÃ©rations",
Â  Â  Â  Â  "last_update": "DerniÃ¨re mise Ã  jour :",
Â  Â  Â  Â  "refresh_btn": "ğŸ”„ ACTUALISER LES DONNÃ‰ES",
Â  Â  Â  Â  "contact": "Contact :",
Â  Â  Â  Â  "filter_title": "ğŸ” Filtres & Recherche AvancÃ©e",
Â  Â  Â  Â  "date_range": "ğŸ“… PÃ©riode",
Â  Â  Â  Â  "clients": "ğŸ‘¤ Clients",
Â  Â  Â  Â  "vehicles": "ğŸš› VÃ©hicules",
Â  Â  Â  Â  "products": "ğŸ“¦ Produits",
Â  Â  Â  Â  "drivers": "ğŸ‘® Chauffeurs",
Â  Â  Â  Â  "global_search": "ğŸ” Recherche Globale (Note, ID...)",
Â  Â  Â  Â  "tab_ops": "ğŸ“ OpÃ©rations",
Â  Â  Â  Â  "tab_ana": "ğŸ“Š Statistiques & Totaux",
Â  Â  Â  Â  "tab_exp": "ğŸ“¤ Exporter SÃ©lection",
Â  Â  Â  Â  "tickets_found": "Tickets TrouvÃ©s",
Â  Â  Â  Â  "doc_preview": "ğŸ“„ AperÃ§u du Document",
Â  Â  Â  Â  "download_pdf": "â¬‡ï¸ TÃ©lÃ©charger PDF",
Â  Â  Â  Â  "pdf_missing": "âš ï¸ PDF Introuvable",
Â  Â  Â  Â  "loading": "Chargement...",
Â  Â  Â  Â  "total_rev": "Total Prix",
Â  Â  Â  Â  "total_weight": "Poids Total",
Â  Â  Â  Â  "count": "Nbr OpÃ©rations",
Â  Â  Â  Â  "curr": "DH",
Â  Â  Â  Â  "weight_unit": "kg"
Â  Â  },
Â  Â  "EN": {
Â  Â  Â  Â  "title": "Operations Dashboard",
Â  Â  Â  Â  "last_update": "Last update:",
Â  Â  Â  Â  "refresh_btn": "ğŸ”„ REFRESH DATA",
Â  Â  Â  Â  "contact": "Contact:",
Â  Â  Â  Â  "filter_title": "ğŸ” Advanced Filters & Search",
Â  Â  Â  Â  "date_range": "ğŸ“… Date Range",
Â  Â  Â  Â  "clients": "ğŸ‘¤ Clients",
Â  Â  Â  Â  "vehicles": "ğŸš› Vehicles",
Â  Â  Â  Â  "products": "ğŸ“¦ Products",
Â  Â  Â  Â  "drivers": "ğŸ‘® Drivers",
Â  Â  Â  Â  "global_search": "ğŸ” Global Search (Note, ID...)",
Â  Â  Â  Â  "tab_ops": "ğŸ“ Operations",
Â  Â  Â  Â  "tab_ana": "ğŸ“Š Statistics & Totals",
Â  Â  Â  Â  "tab_exp": "ğŸ“¤ Export Selection",
Â  Â  Â  Â  "tickets_found": "Tickets Found",
Â  Â  Â  Â  "doc_preview": "ğŸ“„ Document Preview",
Â  Â  Â  Â  "download_pdf": "â¬‡ï¸ Download PDF",
Â  Â  Â  Â  "pdf_missing": "âš ï¸ PDF Not Found",
Â  Â  Â  Â  "loading": "Loading...",
Â  Â  Â  Â  "total_rev": "Total Price",
Â  Â  Â  Â  "total_weight": "Total Weight",
Â  Â  Â  Â  "count": "Total Ops",
Â  Â  Â  Â  "curr": "MAD",
Â  Â  Â  Â  "weight_unit": "kg"
Â  Â  },
Â  Â  "AR": {
Â  Â  Â  Â  "title": "Ù„ÙˆØ­Ø© Ù‚ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª",
Â  Â  Â  Â  "last_update": "Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«:",
Â  Â  Â  Â  "refresh_btn": "ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
Â  Â  Â  Â  "contact": "Ù„Ù„ØªÙˆØ§ØµÙ„:",
Â  Â  Â  Â  "filter_title": "ğŸ” Ø¨Ø­Ø« ÙˆØªØµÙÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©",
Â  Â  Â  Â  "date_range": "ğŸ“… Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©",
Â  Â  Â  Â  "clients": "ğŸ‘¤ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡",
Â  Â  Â  Â  "vehicles": "ğŸš› Ø§Ù„Ù…Ø±ÙƒØ¨Ø§Øª",
Â  Â  Â  Â  "products": "ğŸ“¦ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª",
Â  Â  Â  Â  "drivers": "ğŸ‘® Ø§Ù„Ø³Ø§Ø¦Ù‚ÙŠÙ†",
Â  Â  Â  Â  "global_search": "ğŸ” Ø¨Ø­Ø« Ø´Ø§Ù…Ù„ (Ù…Ù„Ø§Ø­Ø¸Ø§ØªØŒ Ø±Ù‚Ù…...)",
Â  Â  Â  Â  "tab_ops": "ğŸ“ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª",
Â  Â  Â  Â  "tab_ana": "ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø¬Ø§Ù…ÙŠØ¹",
Â  Â  Â  Â  "tab_exp": "ğŸ“¤ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©",
Â  Â  Â  Â  "tickets_found": "Ø§Ù„ØªØ°Ø§ÙƒØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©",
Â  Â  Â  Â  "doc_preview": "ğŸ“„ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©",
Â  Â  Â  Â  "download_pdf": "â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ PDF",
Â  Â  Â  Â  "pdf_missing": "âš ï¸ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯",
Â  Â  Â  Â  "loading": "Ø¬Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„...",
Â  Â  Â  Â  "total_rev": "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¹Ø±",
Â  Â  Â  Â  "total_weight": "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ²Ù†",
Â  Â  Â  Â  "count": "Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª",
Â  Â  Â  Â  "curr": "Ø¯Ø±Ù‡Ù…",
Â  Â  Â  Â  "weight_unit": "ÙƒØº"
Â  Â  }
}

# --- 3. AUTHENTICATION (FIXED FOR SECRETS) ---
@st.cache_resource
def get_drive_service():
Â  Â  """
Â  Â  Connect to Google Drive using either a local 'credentials.json' file
Â  Â  OR Streamlit Secrets (for Cloud).
Â  Â  """
Â  Â  creds = None
Â  Â Â 
Â  Â  # 1. Try Local File (Dev mode)
Â  Â  if os.path.exists('credentials.json'):
Â  Â  Â  Â  creds = service_account.Credentials.from_service_account_file('credentials.json', scopes=SCOPES)
Â  Â  Â  Â Â 
Â  Â  # 2. Try Streamlit Secrets (Cloud mode)
Â  Â  elif 'gcp_service_account' in st.secrets:
Â  Â  Â  Â  # Load the dictionary from secrets.toml
Â  Â  Â  Â  creds_dict = dict(st.secrets["gcp_service_account"])
Â  Â  Â  Â  creds = service_account.Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
Â  Â  Â  Â Â 
Â  Â  # 3. If neither works, stop app
Â  Â  if not creds:
Â  Â  Â  Â  st.error("âŒ Critical Error: No Google Credentials found.")
Â  Â  Â  Â  st.info("If you are running online, please ensure you pasted the keys into Streamlit Secrets.")
Â  Â  Â  Â  st.stop()
Â  Â  Â  Â Â 
Â  Â  return build('drive', 'v3', credentials=creds)

# --- 4. DATA LOADER ---
def normalize_filename(name):
Â  Â  name_no_ext = os.path.splitext(name)[0]
Â  Â  return re.sub(r'[^a-zA-Z0-9]', '', name_no_ext).lower()

@st.cache_data(ttl=600)Â 
def load_data_and_map_files(_service, folder_id):
Â  Â  try:
Â  Â  Â  Â  query = f"'{folder_id}' in parents and trashed=false"
Â  Â  Â  Â  results = _service.files().list(q=query, fields="files(id, name)").execute()
Â  Â  Â  Â  files = results.get('files', [])
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Error connecting to Google Drive: {e}")
Â  Â  Â  Â  return pd.DataFrame()
Â  Â Â 
Â  Â  json_files = [f for f in files if 'json' in f['name'].lower()]
Â  Â  pdf_map = {normalize_filename(f['name']): f['id'] for f in files if 'pdf' in f['name'].lower()}

Â  Â  all_records = []
Â  Â Â 
Â  Â  for file in json_files:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  request = _service.files().get_media(fileId=file['id'])
Â  Â  Â  Â  Â  Â  fh = io.BytesIO()
Â  Â  Â  Â  Â  Â  downloader = MediaIoBaseDownload(fh, request)
Â  Â  Â  Â  Â  Â  done = False
Â  Â  Â  Â  Â  Â  while not done: _, done = downloader.next_chunk()
Â  Â  Â  Â  Â  Â  fh.seek(0)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  content = fh.read().decode('utf-8')
Â  Â  Â  Â  Â  Â  if not content: continue
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  data = json.loads(content)
Â  Â  Â  Â  Â  Â  items = data if isinstance(data, list) else [data]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  for item in items:
Â  Â  Â  Â  Â  Â  Â  Â  # Add Metadata
Â  Â  Â  Â  Â  Â  Â  Â  item['_json_filename'] = file['name']
Â  Â  Â  Â  Â  Â  Â  Â  norm_name = normalize_filename(file['name'])
Â  Â  Â  Â  Â  Â  Â  Â  item['_pdf_file_id'] = pdf_map.get(norm_name)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Fill missing keys
Â  Â  Â  Â  Â  Â  Â  Â  keys_to_check = [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  JSON_KEY_PLATE, JSON_KEY_PRODUCT, JSON_KEY_CLIENT,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  JSON_KEY_DRIVER, JSON_KEY_WEIGHT, JSON_KEY_PRICE,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  JSON_KEY_DATE, JSON_KEY_1, JSON_KEY_2, JSON_KEY_3
Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  for k in keys_to_check:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if k not in item:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  item[k] = ""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  all_records.append(item)
Â  Â  Â  Â  except:
Â  Â  Â  Â  Â  Â  pass
Â  Â  Â  Â  Â  Â Â 
Â  Â  df = pd.DataFrame(all_records)
Â  Â Â 
Â  Â  if not df.empty:
Â  Â  Â  Â  # Clean Date
Â  Â  Â  Â  df['clean_date'] = pd.to_datetime(df[JSON_KEY_DATE], errors='coerce')
Â  Â  Â  Â  if 'date_in' in df.columns:
Â  Â  Â  Â  Â  Â  df['clean_date'] = df['clean_date'].fillna(pd.to_datetime(df['date_in'], errors='coerce'))
Â  Â  Â  Â  df['clean_date'] = df['clean_date'].fillna(pd.to_datetime(datetime.datetime.now()))
Â  Â  Â  Â Â 
Â  Â  Â  Â  df['Hour'] = df['clean_date'].dt.hour
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Clean Price
Â  Â  Â  Â  df['clean_price'] = pd.to_numeric(
Â  Â  Â  Â  Â  Â  df[JSON_KEY_PRICE].astype(str).str.replace(r'[^\d.]', '', regex=True),Â 
Â  Â  Â  Â  Â  Â  errors='coerce'
Â  Â  Â  Â  ).fillna(0)

Â  Â  Â  Â  # Clean Weight
Â  Â  Â  Â  df['clean_weight'] = pd.to_numeric(
Â  Â  Â  Â  Â  Â  df[JSON_KEY_WEIGHT].astype(str).str.replace(r'[^\d.]', '', regex=True),Â 
Â  Â  Â  Â  Â  Â  errors='coerce'
Â  Â  Â  Â  ).fillna(0)

Â  Â  Â  Â  # Standardize Strings
Â  Â  Â  Â  df['Main_Product'] = df[JSON_KEY_PRODUCT].astype(str)
Â  Â  Â  Â  df['Vehicle_Ref'] = df[JSON_KEY_PLATE].astype(str)
Â  Â  Â  Â  df['Client_Ref'] = df[JSON_KEY_CLIENT].astype(str)
Â  Â  Â  Â  df['Driver_Ref'] = df[JSON_KEY_DRIVER].astype(str)
Â  Â  Â  Â  Â  Â Â 
Â  Â  return df

def fetch_pdf_bytes(service, file_id):
Â  Â  try:
Â  Â  Â  Â  request = service.files().get_media(fileId=file_id)
Â  Â  Â  Â  fh = io.BytesIO()
Â  Â  Â  Â  downloader = MediaIoBaseDownload(fh, request)
Â  Â  Â  Â  done = False
Â  Â  Â  Â  while not done: _, done = downloader.next_chunk()
Â  Â  Â  Â  fh.seek(0)
Â  Â  Â  Â  return fh
Â  Â  except:
Â  Â  Â  Â  return None

# --- MAIN APP ---
def main():
Â  Â  # --- SIDEBAR ---
Â  Â  with st.sidebar:
Â  Â  Â  Â  if os.path.exists(LOGO_PATH): st.image(LOGO_PATH, width=160)
Â  Â  Â  Â  st.markdown(f"## {COMPANY_NAME}")
Â  Â  Â  Â  st.markdown(f"ğŸŒ [{WEBSITE_URL}]({WEBSITE_URL})")
Â  Â  Â  Â Â 
Â  Â  Â  Â  lang_code = st.radio("ğŸŒ Language / Ø§Ù„Ù„ØºØ©", ["FranÃ§ais", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", "English"])
Â  Â  Â  Â  if lang_code == "FranÃ§ais": L = TRANSLATIONS["FR"]; is_rtl = False
Â  Â  Â  Â  elif lang_code == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": L = TRANSLATIONS["AR"]; is_rtl = True
Â  Â  Â  Â  else: L = TRANSLATIONS["EN"]; is_rtl = False
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  st.markdown(f"ğŸ“ **{L['contact']}** {PHONE_NUMBER}")
Â  Â  Â  Â  st.divider()

Â  Â  # --- STYLING ---
Â  Â  if is_rtl:
Â  Â  Â  Â  st.markdown("""<style>.stApp { direction: rtl; text-align: right; } .stMarkdown, .stMetric, .stDataFrame, .stExpander, .stTextInput, .stButton {text-align: right !important;}</style>""", unsafe_allow_html=True)
Â  Â  st.markdown("""<style>div[data-testid="stMetricValue"] {font-size: 24px; color: #0068c9;}</style>""", unsafe_allow_html=True)

Â  Â  # --- HEADER & REFRESH ---
Â  Â  col_t1, col_t2 = st.columns([3, 1])
Â  Â  with col_t1:
Â  Â  Â  Â  st.title(L["title"])
Â  Â  with col_t2:
Â  Â  Â  Â  st.write("")Â 
Â  Â  Â  Â  if st.button(L["refresh_btn"], type="primary", use_container_width=True):
Â  Â  Â  Â  Â  Â  st.cache_data.clear()
Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  now_str = datetime.datetime.now().strftime("%H:%M")
Â  Â  st.caption(f"{L['last_update']} {now_str}")

Â  Â  # --- LOAD DATA ---
Â  Â  service = get_drive_service()
Â  Â  raw_df = load_data_and_map_files(service, FOLDER_ID)
Â  Â Â 
Â  Â  if raw_df.empty:
Â  Â  Â  Â  st.info("Waiting for data / En attente de donnÃ©es...")
Â  Â  Â  Â  st.stop()

Â  Â  # Sort Newest First
Â  Â  raw_df = raw_df.sort_values(by='clean_date', ascending=False)

Â  Â  # --- ğŸ” FILTERS ---
Â  Â  with st.expander(L["filter_title"], expanded=True):
Â  Â  Â  Â  c1, c2 = st.columns([1, 2])
Â  Â  Â  Â  min_d = raw_df['clean_date'].min().date() if not raw_df.empty else datetime.date.today()
Â  Â  Â  Â  max_d = raw_df['clean_date'].max().date() if not raw_df.empty else datetime.date.today()
Â  Â  Â  Â Â 
Â  Â  Â  Â  date_range = c1.date_input(L["date_range"], value=(min_d, max_d))
Â  Â  Â  Â  search_term = c2.text_input(L["global_search"], placeholder="...")

Â  Â  Â  Â  c3, c4, c5, c6 = st.columns(4)
Â  Â  Â  Â  sel_clients = c3.multiselect(L["clients"], raw_df['Client_Ref'].unique())
Â  Â  Â  Â  sel_vehs = c4.multiselect(L["vehicles"], raw_df['Vehicle_Ref'].unique())
Â  Â  Â  Â  sel_prods = c5.multiselect(L["products"], raw_df['Main_Product'].unique())
Â  Â  Â  Â  sel_drivers = c6.multiselect(L["drivers"], raw_df['Driver_Ref'].unique())

Â  Â  Â  Â  custom_filters = {}
Â  Â  Â  Â  if UI_NAME_1 or UI_NAME_2 or UI_NAME_3:
Â  Â  Â  Â  Â  Â  st.markdown("---")Â 
Â  Â  Â  Â  Â  Â  cc1, cc2, cc3 = st.columns(3)
Â  Â  Â  Â  Â  Â  if UI_NAME_1 and JSON_KEY_1 in raw_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  vals = [x for x in raw_df[JSON_KEY_1].astype(str).unique() if x and x != "nan"]
Â  Â  Â  Â  Â  Â  Â  Â  custom_filters[JSON_KEY_1] = cc1.multiselect(f"{UI_NAME_1}", vals)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if UI_NAME_2 and JSON_KEY_2 in raw_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  vals = [x for x in raw_df[JSON_KEY_2].astype(str).unique() if x and x != "nan"]
Â  Â  Â  Â  Â  Â  Â  Â  custom_filters[JSON_KEY_2] = cc2.multiselect(f"{UI_NAME_2}", vals)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if UI_NAME_3 and JSON_KEY_3 in raw_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  vals = [x for x in raw_df[JSON_KEY_3].astype(str).unique() if x and x != "nan"]
Â  Â  Â  Â  Â  Â  Â  Â  custom_filters[JSON_KEY_3] = cc3.multiselect(f"{UI_NAME_3}", vals)

Â  Â  # --- APPLY FILTERS ---
Â  Â  filtered_df = raw_df.copy()
Â  Â Â 
Â  Â  if isinstance(date_range, tuple) and len(date_range) == 2:
Â  Â  Â  Â  filtered_df = filtered_df[(filtered_df['clean_date'].dt.date >= date_range[0]) & (filtered_df['clean_date'].dt.date <= date_range[1])]
Â  Â Â 
Â  Â  if sel_clients: filtered_df = filtered_df[filtered_df['Client_Ref'].isin(sel_clients)]
Â  Â  if sel_vehs: filtered_df = filtered_df[filtered_df['Vehicle_Ref'].isin(sel_vehs)]
Â  Â  if sel_prods: filtered_df = filtered_df[filtered_df['Main_Product'].isin(sel_prods)]
Â  Â  if sel_drivers: filtered_df = filtered_df[filtered_df['Driver_Ref'].isin(sel_drivers)]
Â  Â Â 
Â  Â  for key, val in custom_filters.items():
Â  Â  Â  Â  if val: filtered_df = filtered_df[filtered_df[key].astype(str).isin(val)]

Â  Â  if search_term:
Â  Â  Â  Â  mask = filtered_df.astype(str).apply(lambda x: x.str.contains(search_term, case=False)).any(axis=1)
Â  Â  Â  Â  filtered_df = filtered_df[mask]

Â  Â  # --- TABS ---
Â  Â  tab1, tab2, tab3 = st.tabs([L["tab_ops"], L["tab_ana"], L["tab_exp"]])

Â  Â  # === TAB 1: LIST & PDF ===
Â  Â  with tab1:
Â  Â  Â  Â  col_list, col_view = st.columns([1.6, 1])
Â  Â  Â  Â  with col_list:
Â  Â  Â  Â  Â  Â  st.markdown(f"### {L['tickets_found']}: {len(filtered_df)}")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  cols_to_hide = ['clean_date', 'clean_price', 'clean_weight', 'Hour', 'Main_Product', 'Driver_Ref', 'Client_Ref', 'Vehicle_Ref', '_json_filename', '_pdf_file_id']
Â  Â  Â  Â  Â  Â  display_cols = [c for c in filtered_df.columns if not c.startswith('_') and c not in cols_to_hide]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  event = st.dataframe(filtered_df[display_cols], use_container_width=True, on_select="rerun", selection_mode="single-row", hide_index=True, height=600)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  with col_view:
Â  Â  Â  Â  Â  Â  st.markdown(f"### {L['doc_preview']}")
Â  Â  Â  Â  Â  Â  if len(event.selection.rows) > 0:
Â  Â  Â  Â  Â  Â  Â  Â  sel_row = filtered_df.iloc[event.selection.rows[0]]
Â  Â  Â  Â  Â  Â  Â  Â  pdf_id = sel_row.get('_pdf_file_id')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"Ticket: {sel_row.get('ticket_no', 'N/A')} | {sel_row.get('plate', 'N/A')}")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if pdf_id:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(L["loading"]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pdf_fh = fetch_pdf_bytes(service, pdf_id)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pdf_fh:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  b64_pdf = base64.b64encode(pdf_fh.read()).decode('utf-8')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(L["download_pdf"], data=base64.b64decode(b64_pdf), file_name=f"Ticket_{sel_row.get('ticket_no', 'doc')}.pdf", mime="application/pdf", use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f'<iframe src="data:application/pdf;base64,{b64_pdf}" width="100%" height="650px" style="border:none;"></iframe>', unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(L["pdf_missing"])
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("ğŸ‘ˆ")

Â  Â  # === TAB 2: STATISTICS ===
Â  Â  with tab2:
Â  Â  Â  Â  total_ops = len(filtered_df)
Â  Â  Â  Â  total_rev = filtered_df['clean_price'].sum()
Â  Â  Â  Â  total_w = filtered_df['clean_weight'].sum()
Â  Â  Â  Â Â 
Â  Â  Â  Â  m1, m2, m3 = st.columns(3)
Â  Â  Â  Â  m1.metric(L["count"], total_ops)
Â  Â  Â  Â  m2.metric(L["total_rev"], f"{total_rev:,.2f} {L['curr']}")
Â  Â  Â  Â  m3.metric(L["total_weight"], f"{total_w:,.2f} {L['weight_unit']}")
Â  Â  Â  Â  st.divider()
Â  Â  Â  Â Â 
Â  Â  Â  Â  c1, c2 = st.columns(2)
Â  Â  Â  Â  traffic_df = filtered_df.dropna(subset=['clean_date'])
Â  Â  Â  Â  if not traffic_df.empty:
Â  Â  Â  Â  Â  Â  vol = traffic_df.groupby('Hour').size().reset_index(name='Cnt')
Â  Â  Â  Â  Â  Â  fig1 = px.bar(vol, x='Hour', y='Cnt', title="Traffic (Hour)", color_discrete_sequence=['#2ecc71'])
Â  Â  Â  Â  Â  Â  c1.plotly_chart(fig1, use_container_width=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not filtered_df['Main_Product'].empty:
Â  Â  Â  Â  Â  Â  top_prod = filtered_df['Main_Product'].value_counts().nlargest(5).reset_index()
Â  Â  Â  Â  Â  Â  top_prod.columns = ['Product', 'Count']
Â  Â  Â  Â  Â  Â  fig2 = px.pie(top_prod, names='Product', values='Count', title="Top Products", hole=0.4)
Â  Â  Â  Â  Â  Â  c2.plotly_chart(fig2, use_container_width=True)

Â  Â  # === TAB 3: EXPORT ===
Â  Â  with tab3:
Â  Â  Â  Â  st.markdown(f"### {L['tab_exp']}")
Â  Â  Â  Â  if st.button("Generate Excel"):
Â  Â  Â  Â  Â  Â  output = io.BytesIO()
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filtered_df.drop(columns=[c for c in filtered_df.columns if c.startswith('_')]).to_excel(writer, index=False)
Â  Â  Â  Â  Â  Â  except:
Â  Â  Â  Â  Â  Â  Â  Â  with pd.ExcelWriter(output, engine='openpyxl') as writer:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filtered_df.drop(columns=[c for c in filtered_df.columns if c.startswith('_')]).to_excel(writer, index=False)
Â  Â  Â  Â  Â  Â  st.download_button("ğŸ“¥ Download.xlsx", data=output.getvalue(), file_name=f"Report_{datetime.date.today()}.xlsx")

if __name__ == "__main__":
Â  Â  main()

