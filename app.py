import streamlit as st
import pandas as pd
import json
import io
import base64
import os
import re
import datetime
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import plotly.express as px

# --- 1. CONFIGURATION & BRANDING ---
COMPANY_NAME = "Orient Scale Solutions"
WEBSITE_URL = "https://www.orientscale.ma"
PHONE_NUMBER = "0661572700"
LOGO_PATH = "logo.png"

# --- ğŸ› ï¸ JSON KEY MAPPING ğŸ› ï¸ ---
JSON_KEY_PLATE = "plate"
JSON_KEY_PRODUCT = "product"
JSON_KEY_CLIENT = "client"
JSON_KEY_DRIVER = "driver"
JSON_KEY_WEIGHT = "net"
JSON_KEY_PRICE = "price"
JSON_KEY_DATE = "date_out"

# --- ğŸ› ï¸ CUSTOM FIELDS CONFIGURATION ğŸ› ï¸ ---
UI_NAME_1 = "Destination"
JSON_KEY_1 = "ex1"

UI_NAME_2 = "Source"
JSON_KEY_2 = "ex2"

UI_NAME_3 = "Remorque"
JSON_KEY_3 = "ex3"

# Drive Config
FOLDER_ID = "115OinwLcQMYZ2l50qMEACnCt-9pXiy1o"
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']

# --- PAGE CONFIG ---
st.set_page_config(page_title=COMPANY_NAME, layout="wide", page_icon="âš–ï¸")

# --- 2. TRANSLATION DICTIONARY ---
TRANSLATIONS = {
    "FR": {
        "title": "Tableau de Bord des OpÃ©rations",
        "last_update": "DerniÃ¨re mise Ã  jour :",
        "refresh_btn": "ğŸ”„ ACTUALISER LES DONNÃ‰ES",
        "contact": "Contact :",
        "filter_title": "ğŸ” Filtres & Recherche AvancÃ©e",
        "date_range": "ğŸ“… PÃ©riode",
        "clients": "ğŸ‘¤ Clients",
        "vehicles": "ğŸš› VÃ©hicules",
        "products": "ğŸ“¦ Produits",
        "drivers": "ğŸ‘® Chauffeurs",
        "global_search": "ğŸ” Recherche Globale (Note, ID...)",
        "tab_ops": "ğŸ“ OpÃ©rations",
        "tab_ana": "ğŸ“Š Statistiques & Totaux",
        "tab_exp": "ğŸ“¤ Exporter SÃ©lection",
        "tickets_found": "Tickets TrouvÃ©s",
        "doc_preview": "ğŸ“„ AperÃ§u du Document",
        "download_pdf": "â¬‡ï¸ TÃ©lÃ©charger PDF",
        "pdf_missing": "âš ï¸ PDF Introuvable",
        "loading": "Chargement...",
        "total_rev": "Total Prix",
        "total_weight": "Poids Total",
        "count": "Nbr OpÃ©rations",
        "curr": "DH",
        "weight_unit": "kg"
    },
    "EN": {
        "title": "Operations Dashboard",
        "last_update": "Last update:",
        "refresh_btn": "ğŸ”„ REFRESH DATA",
        "contact": "Contact:",
        "filter_title": "ğŸ” Advanced Filters & Search",
        "date_range": "ğŸ“… Date Range",
        "clients": "ğŸ‘¤ Clients",
        "vehicles": "ğŸš› Vehicles",
        "products": "ğŸ“¦ Products",
        "drivers": "ğŸ‘® Drivers",
        "global_search": "ğŸ” Global Search (Note, ID...)",
        "tab_ops": "ğŸ“ Operations",
        "tab_ana": "ğŸ“Š Statistics & Totals",
        "tab_exp": "ğŸ“¤ Export Selection",
        "tickets_found": "Tickets Found",
        "doc_preview": "ğŸ“„ Document Preview",
        "download_pdf": "â¬‡ï¸ Download PDF",
        "pdf_missing": "âš ï¸ PDF Not Found",
        "loading": "Loading...",
        "total_rev": "Total Price",
        "total_weight": "Total Weight",
        "count": "Total Ops",
        "curr": "MAD",
        "weight_unit": "kg"
    },
    "AR": {
        "title": "Ù„ÙˆØ­Ø© Ù‚ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª",
        "last_update": "Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«:",
        "refresh_btn": "ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
        "contact": "Ù„Ù„ØªÙˆØ§ØµÙ„:",
        "filter_title": "ğŸ” Ø¨Ø­Ø« ÙˆØªØµÙÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©",
        "date_range": "ğŸ“… Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©",
        "clients": "ğŸ‘¤ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡",
        "vehicles": "ğŸš› Ø§Ù„Ù…Ø±ÙƒØ¨Ø§Øª",
        "products": "ğŸ“¦ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª",
        "drivers": "ğŸ‘® Ø§Ù„Ø³Ø§Ø¦Ù‚ÙŠÙ†",
        "global_search": "ğŸ” Ø¨Ø­Ø« Ø´Ø§Ù…Ù„ (Ù…Ù„Ø§Ø­Ø¸Ø§ØªØŒ Ø±Ù‚Ù…...)",
        "tab_ops": "ğŸ“ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª",
        "tab_ana": "ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø¬Ø§Ù…ÙŠØ¹",
        "tab_exp": "ğŸ“¤ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©",
        "tickets_found": "Ø§Ù„ØªØ°Ø§ÙƒØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©",
        "doc_preview": "ğŸ“„ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©",
        "download_pdf": "â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ PDF",
        "pdf_missing": "âš ï¸ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯",
        "loading": "Ø¬Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„...",
        "total_rev": "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¹Ø±",
        "total_weight": "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ²Ù†",
        "count": "Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª",
        "curr": "Ø¯Ø±Ù‡Ù…",
        "weight_unit": "ÙƒØº"
    }
}

# --- 3. AUTHENTICATION (FIXED FOR SECRETS) ---
@st.cache_resource
def get_drive_service():
    """
    Connect to Google Drive using either a local 'credentials.json' file
    OR Streamlit Secrets (for Cloud).
    """
    creds = None
    
    # 1. Try Local File (Dev mode)
    if os.path.exists('credentials.json'):
        creds = service_account.Credentials.from_service_account_file('credentials.json', scopes=SCOPES)
        
    # 2. Try Streamlit Secrets (Cloud mode)
    elif 'gcp_service_account' in st.secrets:
        # Load the dictionary from secrets.toml
        creds_dict = dict(st.secrets["gcp_service_account"])
        creds = service_account.Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
        
    # 3. If neither works, stop app
    if not creds:
        st.error("âŒ Critical Error: No Google Credentials found.")
        st.info("If you are running online, please ensure you pasted the keys into Streamlit Secrets.")
        st.stop()
        
    return build('drive', 'v3', credentials=creds)

# --- 4. DATA LOADER ---
def normalize_filename(name):
    name_no_ext = os.path.splitext(name)[0]
    return re.sub(r'[^a-zA-Z0-9]', '', name_no_ext).lower()

@st.cache_data(ttl=600) 
def load_data_and_map_files(_service, folder_id):
    try:
        query = f"'{folder_id}' in parents and trashed=false"
        results = _service.files().list(q=query, fields="files(id, name)").execute()
        files = results.get('files', [])
    except Exception as e:
        st.error(f"Error connecting to Google Drive: {e}")
        return pd.DataFrame()
    
    json_files = [f for f in files if 'json' in f['name'].lower()]
    pdf_map = {normalize_filename(f['name']): f['id'] for f in files if 'pdf' in f['name'].lower()}

    all_records = []
    
    for file in json_files:
        try:
            request = _service.files().get_media(fileId=file['id'])
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while not done: _, done = downloader.next_chunk()
            fh.seek(0)
            
            content = fh.read().decode('utf-8')
            if not content: continue
            
            data = json.loads(content)
            items = data if isinstance(data, list) else [data]
            
            for item in items:
                # Add Metadata
                item['_json_filename'] = file['name']
                norm_name = normalize_filename(file['name'])
                item['_pdf_file_id'] = pdf_map.get(norm_name)
                
                # Fill missing keys
                keys_to_check = [
                    JSON_KEY_PLATE, JSON_KEY_PRODUCT, JSON_KEY_CLIENT, 
                    JSON_KEY_DRIVER, JSON_KEY_WEIGHT, JSON_KEY_PRICE, 
                    JSON_KEY_DATE, JSON_KEY_1, JSON_KEY_2, JSON_KEY_3
                ]
                for k in keys_to_check:
                    if k not in item:
                        item[k] = ""
                        
                all_records.append(item)
        except:
            pass
            
    df = pd.DataFrame(all_records)
    
    if not df.empty:
        # Clean Date
        df['clean_date'] = pd.to_datetime(df[JSON_KEY_DATE], errors='coerce')
        if 'date_in' in df.columns:
            df['clean_date'] = df['clean_date'].fillna(pd.to_datetime(df['date_in'], errors='coerce'))
        df['clean_date'] = df['clean_date'].fillna(pd.to_datetime(datetime.datetime.now()))
        
        df['Hour'] = df['clean_date'].dt.hour
        
        # Clean Price
        df['clean_price'] = pd.to_numeric(
            df[JSON_KEY_PRICE].astype(str).str.replace(r'[^\d.]', '', regex=True), 
            errors='coerce'
        ).fillna(0)

        # Clean Weight
        df['clean_weight'] = pd.to_numeric(
            df[JSON_KEY_WEIGHT].astype(str).str.replace(r'[^\d.]', '', regex=True), 
            errors='coerce'
        ).fillna(0)

        # Standardize Strings
        df['Main_Product'] = df[JSON_KEY_PRODUCT].astype(str)
        df['Vehicle_Ref'] = df[JSON_KEY_PLATE].astype(str)
        df['Client_Ref'] = df[JSON_KEY_CLIENT].astype(str)
        df['Driver_Ref'] = df[JSON_KEY_DRIVER].astype(str)
            
    return df

def fetch_pdf_bytes(service, file_id):
    try:
        request = service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done: _, done = downloader.next_chunk()
        fh.seek(0)
        return fh
    except:
        return None

# --- MAIN APP ---
def main():
    # --- SIDEBAR ---
    with st.sidebar:
        if os.path.exists(LOGO_PATH): st.image(LOGO_PATH, width=160)
        st.markdown(f"## {COMPANY_NAME}")
        st.markdown(f"ğŸŒ [{WEBSITE_URL}]({WEBSITE_URL})")
        
        lang_code = st.radio("ğŸŒ Language / Ø§Ù„Ù„ØºØ©", ["FranÃ§ais", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", "English"])
        if lang_code == "FranÃ§ais": L = TRANSLATIONS["FR"]; is_rtl = False
        elif lang_code == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": L = TRANSLATIONS["AR"]; is_rtl = True
        else: L = TRANSLATIONS["EN"]; is_rtl = False
            
        st.markdown(f"ğŸ“ **{L['contact']}** {PHONE_NUMBER}")
        st.divider()

    # --- STYLING ---
    if is_rtl:
        st.markdown("""<style>.stApp { direction: rtl; text-align: right; } .stMarkdown, .stMetric, .stDataFrame, .stExpander, .stTextInput, .stButton {text-align: right !important;}</style>""", unsafe_allow_html=True)
    st.markdown("""<style>div[data-testid="stMetricValue"] {font-size: 24px; color: #0068c9;}</style>""", unsafe_allow_html=True)

    # --- HEADER & REFRESH ---
    col_t1, col_t2 = st.columns([3, 1])
    with col_t1:
        st.title(L["title"])
    with col_t2:
        st.write("") 
        if st.button(L["refresh_btn"], type="primary", use_container_width=True):
            st.cache_data.clear()
            st.rerun()

    now_str = datetime.datetime.now().strftime("%H:%M")
    st.caption(f"{L['last_update']} {now_str}")

    # --- LOAD DATA ---
    service = get_drive_service()
    raw_df = load_data_and_map_files(service, FOLDER_ID)
    
    if raw_df.empty:
        st.info("Waiting for data / En attente de donnÃ©es...")
        st.stop()

    # Sort Newest First
    raw_df = raw_df.sort_values(by='clean_date', ascending=False)

    # --- ğŸ” FILTERS ---
    with st.expander(L["filter_title"], expanded=True):
        c1, c2 = st.columns([1, 2])
        min_d = raw_df['clean_date'].min().date() if not raw_df.empty else datetime.date.today()
        max_d = raw_df['clean_date'].max().date() if not raw_df.empty else datetime.date.today()
        
        date_range = c1.date_input(L["date_range"], value=(min_d, max_d))
        search_term = c2.text_input(L["global_search"], placeholder="...")

        c3, c4, c5, c6 = st.columns(4)
        sel_clients = c3.multiselect(L["clients"], raw_df['Client_Ref'].unique())
        sel_vehs = c4.multiselect(L["vehicles"], raw_df['Vehicle_Ref'].unique())
        sel_prods = c5.multiselect(L["products"], raw_df['Main_Product'].unique())
        sel_drivers = c6.multiselect(L["drivers"], raw_df['Driver_Ref'].unique())

        custom_filters = {}
        if UI_NAME_1 or UI_NAME_2 or UI_NAME_3:
            st.markdown("---") 
            cc1, cc2, cc3 = st.columns(3)
            if UI_NAME_1 and JSON_KEY_1 in raw_df.columns:
                vals = [x for x in raw_df[JSON_KEY_1].astype(str).unique() if x and x != "nan"]
                custom_filters[JSON_KEY_1] = cc1.multiselect(f"{UI_NAME_1}", vals)
            
            if UI_NAME_2 and JSON_KEY_2 in raw_df.columns:
                vals = [x for x in raw_df[JSON_KEY_2].astype(str).unique() if x and x != "nan"]
                custom_filters[JSON_KEY_2] = cc2.multiselect(f"{UI_NAME_2}", vals)
            
            if UI_NAME_3 and JSON_KEY_3 in raw_df.columns:
                vals = [x for x in raw_df[JSON_KEY_3].astype(str).unique() if x and x != "nan"]
                custom_filters[JSON_KEY_3] = cc3.multiselect(f"{UI_NAME_3}", vals)

    # --- APPLY FILTERS ---
    filtered_df = raw_df.copy()
    
    if isinstance(date_range, tuple) and len(date_range) == 2:
        filtered_df = filtered_df[(filtered_df['clean_date'].dt.date >= date_range[0]) & (filtered_df['clean_date'].dt.date <= date_range[1])]
    
    if sel_clients: filtered_df = filtered_df[filtered_df['Client_Ref'].isin(sel_clients)]
    if sel_vehs: filtered_df = filtered_df[filtered_df['Vehicle_Ref'].isin(sel_vehs)]
    if sel_prods: filtered_df = filtered_df[filtered_df['Main_Product'].isin(sel_prods)]
    if sel_drivers: filtered_df = filtered_df[filtered_df['Driver_Ref'].isin(sel_drivers)]
    
    for key, val in custom_filters.items():
        if val: filtered_df = filtered_df[filtered_df[key].astype(str).isin(val)]

    if search_term:
        mask = filtered_df.astype(str).apply(lambda x: x.str.contains(search_term, case=False)).any(axis=1)
        filtered_df = filtered_df[mask]

    # --- TABS ---
    tab1, tab2, tab3 = st.tabs([L["tab_ops"], L["tab_ana"], L["tab_exp"]])

    # === TAB 1: LIST & PDF ===
    with tab1:
        col_list, col_view = st.columns([1.6, 1])
        with col_list:
            st.markdown(f"### {L['tickets_found']}: {len(filtered_df)}")
            
            cols_to_hide = ['clean_date', 'clean_price', 'clean_weight', 'Hour', 'Main_Product', 'Driver_Ref', 'Client_Ref', 'Vehicle_Ref', '_json_filename', '_pdf_file_id']
            display_cols = [c for c in filtered_df.columns if not c.startswith('_') and c not in cols_to_hide]
            
            event = st.dataframe(filtered_df[display_cols], use_container_width=True, on_select="rerun", selection_mode="single-row", hide_index=True, height=600)
            
        with col_view:
            st.markdown(f"### {L['doc_preview']}")
            if len(event.selection.rows) > 0:
                sel_row = filtered_df.iloc[event.selection.rows[0]]
                pdf_id = sel_row.get('_pdf_file_id')
                
                st.info(f"Ticket: {sel_row.get('ticket_no', 'N/A')} | {sel_row.get('plate', 'N/A')}")
                
                if pdf_id:
                    with st.spinner(L["loading"]):
                        pdf_fh = fetch_pdf_bytes(service, pdf_id)
                    if pdf_fh:
                        b64_pdf = base64.b64encode(pdf_fh.read()).decode('utf-8')
                        st.download_button(L["download_pdf"], data=base64.b64decode(b64_pdf), file_name=f"Ticket_{sel_row.get('ticket_no', 'doc')}.pdf", mime="application/pdf", use_container_width=True)
                        st.markdown(f'<iframe src="data:application/pdf;base64,{b64_pdf}" width="100%" height="650px" style="border:none;"></iframe>', unsafe_allow_html=True)
                else:
                    st.warning(L["pdf_missing"])
            else:
                st.info("ğŸ‘ˆ")

    # === TAB 2: STATISTICS ===
    with tab2:
        total_ops = len(filtered_df)
        total_rev = filtered_df['clean_price'].sum()
        total_w = filtered_df['clean_weight'].sum()
        
        m1, m2, m3 = st.columns(3)
        m1.metric(L["count"], total_ops)
        m2.metric(L["total_rev"], f"{total_rev:,.2f} {L['curr']}")
        m3.metric(L["total_weight"], f"{total_w:,.2f} {L['weight_unit']}")
        st.divider()
        
        c1, c2 = st.columns(2)
        traffic_df = filtered_df.dropna(subset=['clean_date'])
        if not traffic_df.empty:
            vol = traffic_df.groupby('Hour').size().reset_index(name='Cnt')
            fig1 = px.bar(vol, x='Hour', y='Cnt', title="Traffic (Hour)", color_discrete_sequence=['#2ecc71'])
            c1.plotly_chart(fig1, use_container_width=True)
        
        if not filtered_df['Main_Product'].empty:
            top_prod = filtered_df['Main_Product'].value_counts().nlargest(5).reset_index()
            top_prod.columns = ['Product', 'Count']
            fig2 = px.pie(top_prod, names='Product', values='Count', title="Top Products", hole=0.4)
            c2.plotly_chart(fig2, use_container_width=True)

    # === TAB 3: EXPORT ===
    with tab3:
        st.markdown(f"### {L['tab_exp']}")
        if st.button("Generate Excel"):
            output = io.BytesIO()
            try:
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    filtered_df.drop(columns=[c for c in filtered_df.columns if c.startswith('_')]).to_excel(writer, index=False)
            except:
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    filtered_df.drop(columns=[c for c in filtered_df.columns if c.startswith('_')]).to_excel(writer, index=False)
            st.download_button("ğŸ“¥ Download.xlsx", data=output.getvalue(), file_name=f"Report_{datetime.date.today()}.xlsx")

if __name__ == "__main__":
    main()